# (optional) The directory where models and summaries will be saved.
# Can also be set with the command line option --model_dir.
# The directory is created if it does not exist.
model_dir: toy-ende

# (optional) Enable automatic parameters based on the selected model.
# Can also be set with the command line option --auto_config.
auto_config: true

data:
  # (required for train run type).
  train_features_file: data/toy-ende/src-train.txt
  train_labels_file: data/toy-ende/tgt-train.txt

  # (optional) A list with the weights of each training files, if multiple training
  # files were configured (default: null).
  train_files_weights: null

  # (optional) Pharaoh alignments of the training files.
  train_alignments: data/toy-ende/alignments-train.txt

  # (optional) File containing the weight of each example (one weight per line).
  # The loss value of each example is multiplied by its corresponding weight.
  example_weights: data/toy-ende/weights-train.txt

  # (required for train_end_eval and eval run types).
  eval_features_file: data/toy-ende/src-val.txt
  eval_labels_file: data/toy-ende/tgt-val.txt

  # (optional) Models may require additional resource files (e.g. vocabularies).
  source_vocabulary: data/toy-ende/src-vocab.txt
  target_vocabulary: data/toy-ende/tgt-vocab.txt

  # (optional) During export save the vocabularies as model assets, otherwise embed
  # them in the graph itself (default: true).
  export_vocabulary_assets: true

  # (optional) Tokenization configuration (or path to a configuration file).
  # See also: https://github.com/OpenNMT/Tokenizer/blob/master/docs/options.md
  source_tokenization:
    type: OpenNMTTokenizer
    params:
      mode: aggressive
      joiner_annotate: true
      segment_numbers: true
      segment_alphabet_change: true
  target_tokenization: config/tokenization/aggressive.yml

  # (optional) Pretrained embedding configuration.
  source_embedding:
    path: data/glove/glove-100000.txt
    with_header: true
    case_insensitive: true
    trainable: false

  # (optional) For language models, configure sequence control tokens (usually
  # represented as <s> and </s>). For example, enabling "start" and disabling "end"
  # allows nonconditional and unbounded generation (default: start=false, end=true).
  #
  # Advanced users could also configure this parameter for seq2seq models with e.g.
  # source_sequence_controls and target_sequence_controls.
  sequence_controls:
    start: false
    end: true

  # (optional) For sequence tagging tasks, the tagging scheme that is used (e.g. BIOES).
  # For supported schemes, additional evaluation metrics could be computed such as
  # precision, recall, etc. (accepted values: bioes; default: null).
  tagging_scheme: bioes

# Model and optimization parameters.
params:
  # The optimizer class name in tf.keras.optimizers or tfa.optimizers.
  optimizer: Adam
  # (optional) Additional optimizer parameters as defined in their documentation.
  # If weight_decay is set, the optimizer will be extended with decoupled weight decay.
  optimizer_params:
    beta_1: 0.8
    beta_2: 0.998
  learning_rate: 1.0

  # (optional) If set, overrides all dropout values configured in the model definition.
  dropout: 0.3

  # (optional) List of layer to not optimize.
  freeze_layers:
    - "encoder/layers/0"
    - "decoder/output_layer"

  # (optional) Weights regularization penalty (default: null).
  regularization:
    type: l2  # can be "l1", "l2", "l1_l2" (case-insensitive).
    scale: 1e-4  # if using "l1_l2" regularization, this should be a YAML list.

  # (optional) Average loss in the time dimension in addition to the batch dimension
  # (default: true when using "tokens" batch type, false otherwise).
  average_loss_in_time: false
  # (optional) High training loss values considered as outliers will be masked (default: false).
  mask_loss_outliers: false

  # (optional) The type of learning rate decay (default: null). See:
  #  * https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules
  #  * https://opennmt.net/OpenNMT-tf/package/opennmt.schedules.html
  # This value may change the semantics of other decay options. See the documentation
  # or the code.
  decay_type: NoamDecay
  # (optional unless decay_type is set) Decay parameters.
  decay_params:
    model_dim: 512
    warmup_steps: 4000
  # (optional) The number of training steps that make 1 decay step (default: 1).
  decay_step_duration: 1
  # (optional) After how many steps to start the decay (default: 0).
  start_decay_steps: 50000

  # (optional) The learning rate minimum value (default: 0).
  minimum_learning_rate: 0.0001

  # (optional) Type of scheduled sampling (can be "constant", "linear", "exponential",
  # or "inverse_sigmoid", default: "constant").
  scheduled_sampling_type: constant
  # (optional) Probability to read directly from the inputs instead of sampling categorically
  # from the output ids (default: 1).
  scheduled_sampling_read_probability: 1
  # (optional unless scheduled_sampling_type is set) The constant k of the schedule.
  scheduled_sampling_k: 0

  # (optional) The label smoothing value.
  label_smoothing: 0.1

  # (optional) Width of the beam search (default: 1).
  beam_width: 5
  # (optional) Number of hypotheses to return (default: 1). Set 0 to return all
  # available hypotheses. This value is also set by infer/n_best.
  num_hypotheses: 1
  # (optional) Length penaly weight to use during beam search (default: 0).
  length_penalty: 0.2
  # (optional) Coverage penaly weight to use during beam search (default: 0).
  coverage_penalty: 0.2
  # (optional) Sample predictions from the top K most likely tokens (requires
  # beam_width to 1). If 0, sample from the full output distribution (default: 1).
  sampling_topk: 1
  # (optional) High temperatures generate more random samples (default: 1).
  sampling_temperature: 1
  # (optional) Sequence of noise to apply to the decoding output. Each element
  # should be a noise type (can be: "dropout", "replacement", "permutation") and
  # the module arguments
  # (see https://opennmt.net/OpenNMT-tf/package/opennmt.data.noise.html)
  decoding_noise:
    - dropout: 0.1
    - replacement: [0.1, ｟unk｠]
    - permutation: 3
  # (optional) Define the subword marker. This is useful to apply noise at the
  # word level instead of the subword level (default: ￭).
  decoding_subword_token: ￭
  # (optional) Whether decoding_subword_token is used as a spacer (as in SentencePiece)
  # or a joiner (as in BPE).
  # If unspecified, will infer  directly from decoding_subword_token.
  decoding_subword_token_is_spacer: false
  # (optional) Minimum length of decoded sequences, end token excluded (default: 0).
  minimum_decoding_length: 0
  # (optional) Maximum length of decoded sequences, end token excluded (default: 250).
  maximum_decoding_length: 250

  # (optional) Replace unknown target tokens by the original source token with the
  # highest attention (default: false).
  replace_unknown_target: false

  # (optional) The type of guided alignment cost to compute (can be: "null", "ce", "mse",
  # default: "null").
  guided_alignment_type: null
  # (optional) The weight of the guided alignment cost (default: 1).
  guided_alignment_weight: 1

  # (optional) Enable contrastive learning mode, see
  # https://www.aclweb.org/anthology/P19-1623 (default: false).
  # See also "decoding_subword_token" that is used by this mode.
  contrastive_learning: false
  # (optional) The value of the parameter eta in the max-margin loss (default: 0.1).
  max_margin_eta: 0.1
  # (optional) Size of output on an exported TensorFlow Lite model
  tflite_output_size: 250


# Training options.
train:
  # (optional) Training batch size. If set to 0, the training will search the largest
  # possible batch size.
  batch_size: 64
  # (optional) Batch size is the number of "examples" or "tokens" (default: "examples").
  batch_type: examples
  # (optional) Tune gradient accumulation to train with at least this effective batch size
  # (default: null).
  effective_batch_size: 25000

  # (optional) Save a checkpoint every this many steps (default: 5000).
  save_checkpoints_steps: null
  # (optional) How many checkpoints to keep on disk.
  keep_checkpoint_max: 3

  # (optional) Dump summaries and logs every this many steps (default: 100).
  save_summary_steps: 100

  # (optional) Maximum training step. If not set, train forever.
  max_step: 1000000
  # (optional) If true, makes a single pass over the training data (default: false).
  single_pass: false

  # (optional) The maximum length of feature sequences during training (default: null).
  maximum_features_length: 70
  # (optional) The maximum length of label sequences during training (default: null).
  maximum_labels_length: 70

  # (optional) The width of the length buckets to select batch candidates from.
  # A smaller value means less padding and increased efficiency. (default: 1).
  length_bucket_width: 1

  # (optional) The number of elements from which to sample during shuffling (default: 500000).
  # Set 0 or null to disable shuffling, -1 to match the number of training examples.
  sample_buffer_size: 500000

  # (optional) Moving average decay. Reasonable values are close to 1, e.g. 0.9999, see
  # https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage
  # (default: null)
  moving_average_decay: 0.9999
  # (optional) Number of checkpoints to average at the end of the training to the directory
  # model_dir/avg (default: 0).
  average_last_checkpoints: 8


# (optional) Evaluation options.
eval:
  # (optional) The batch size to use (default: 32).
  batch_size: 30
  # (optional) Batch size is the number of "examples" or "tokens" (default: "examples").
  batch_type: examples

  # (optional) Evaluate every this many steps (default: 5000).
  steps: 5000

  # (optional) Save evaluation predictions in model_dir/eval/.
  save_eval_predictions: false
  # (optional) Scorer or list of scorers that are called on the saved evaluation
  # predictions.
  # Available scorers: bleu, rouge, wer, ter, prf, chrf, chrf++
  scorers: bleu

  # (optional) The width of the length buckets to select batch candidates from.
  # If set, the eval data will be sorted by length to increase the translation
  # efficiency. The predictions will still be outputted in order as they are
  # available (default: 0).
  length_bucket_width: 5

  # (optional) Export a model when a metric has the best value so far (default: null).
  export_on_best: bleu
  # (optional) Format of the exported model (can be: "saved_model, "checkpoint",
  # "ctranslate2", "ctranslate2_int8", "ctranslate2_int16", "ctranslate2_float16",
  # default: "saved_model").
  export_format: saved_model
  # (optional) Maximum number of exports to keep on disk (default: 5).
  max_exports_to_keep: 5

  # (optional) Early stopping condition.
  # Should be read as: stop the training if "metric" did not improve more
  # than "min_improvement" in the last "steps" evaluations.
  early_stopping:
    # (optional) The target metric name (default: "loss").
    metric: bleu
    # (optional) The metric should improve at least by this much to be considered
    # as an improvement (default: 0)
    min_improvement: 0.01
    steps: 4

# (optional) Inference options.
infer:
  # (optional) The batch size to use (default: 16).
  batch_size: 10
  # (optional) Batch size is the number of "examples" or "tokens" (default: "examples").
  batch_type: examples

  # (optional) For compatible models, the number of hypotheses to output (default: 1).
  # This sets the parameter params/num_hypotheses.
  n_best: 1
  # (optional) For compatible models, also output the score (default: false).
  with_scores: false
  # (optional) For compatible models, also output the alignments
  # (can be: null, hard, soft, default: null).
  with_alignments: null

  # (optional) The width of the length buckets to select batch candidates from.
  # If set, the test data will be sorted by length to increase the translation
  # efficiency. The predictions will still be outputted in order as they are
  # available (default: 0).
  length_bucket_width: 5


# (optional) Scoring options.
score:
  # (optional) The batch size to use (default: 64).
  batch_size: 64
  # (optional) Batch size is the number of "examples" or "tokens" (default: "examples").
  batch_type: examples

  # (optional) The width of the length buckets to select batch candidates from.
  # If set, the input file will be sorted by length to increase efficiency.
  # The result will still be outputted in order as they are available (default: 0).
  length_bucket_width: 0

  # (optional) Also report token-level cross entropy.
  with_token_level: false
  # (optional) Also output the alignments (can be: null, hard, soft, default: null).
  with_alignments: null


Namespace(config=None, 
save_config=None, 
data='/home/zk274707/tutos/translation_WMT17/OpenNMT-py/data/data.yaml', 
skip_empty_level='warning', 
transforms=[], 
save_data=None, 
overwrite=False, 
n_sample=0, 
dump_transforms=False, 
src_vocab='/home/zk274707/tutos/translation_WMT17/OpenNMT-py/docs/source/examples/wmt17_en_de/vocab.shared', 
tgt_vocab=None, 
share_vocab=True, 
decoder_start_token='<s>', 
default_specials=['<unk>', '<blank>', '<s>', '</s>'], 
n_src_feats=0, 
src_feats_defaults=None, 
src_vocab_size=32768, 
tgt_vocab_size=32768, 
vocab_size_multiple=8, 
src_words_min_frequency=0, 
tgt_words_min_frequency=0, 
src_seq_length_trunc=None, 
tgt_seq_length_trunc=None, 
both_embeddings=None, 
src_embeddings=None, 
tgt_embeddings=None, 
embeddings_type=None, 
reversible_tokenization='joiner', 
src_seq_length=192, 
tgt_seq_length=192, 
src_prefix='', 
tgt_prefix='', 
src_suffix='', 
tgt_suffix='', 
upper_corpus_ratio=0.01, 
src_subword_model=None, 
tgt_subword_model=None, 
src_subword_nbest=1, 
tgt_subword_nbest=1, 
src_subword_alpha=0, 
tgt_subword_alpha=0, 
src_subword_vocab='', 
tgt_subword_vocab='', 
src_vocab_threshold=0, 
tgt_vocab_threshold=0, 
src_subword_type='none', 
tgt_subword_type='none', 
src_onmttok_kwargs="{'mode': 'none'}", 
tgt_onmttok_kwargs="{'mode': 'none'}", 
gpt2_pretok=False, 
switchout_temperature=1.0, 
tokendrop_temperature=1.0, 
tokenmask_temperature=1.0, 
doc_length=200, 
max_context=1, 
src_eq_tgt=False, 
same_char=False, 
same_word=False, 
scripts_ok=['Latin', 'Common'], 
scripts_nok=[], 
src_tgt_ratio=2,
avg_tok_min=3, 
avg_tok_max=20, 
langid=[], 
tags_dictionary_path=None, 
tags_corpus_ratio=0.1, 
max_tags=12, 
paired_stag='｟ph_#_beg｠', 
paired_etag='｟ph_#_end｠', 
isolated_tag='｟ph_#_std｠', 
src_delimiter='｟fuzzy｠', 
tm_path=None, 
fuzzy_corpus_ratio=0.1, 
fuzzy_threshold=70, 
tm_delimiter='\t', 
fuzzy_token='｟fuzzy｠', 
fuzzymatch_min_length=4, 
fuzzymatch_max_length=70, 
permute_sent_ratio=0.0, 
rotate_ratio=0.0, 
insert_ratio=0.0, 
random_ratio=0.0, 
mask_ratio=0.0, 
mask_length='subword', 
poisson_lambda=3.0, 
replace_length=-1, 
src_lang='', 
tgt_lang='', 
penn=True, 
norm_quote_commas=True, 
norm_numbers=True, 
pre_replace_unicode_punct=False, 
post_remove_control_chars=False, 
src_word_vec_size=500, 
tgt_word_vec_size=500, 
word_vec_size=-1, 
share_decoder_embeddings=False, 
share_embeddings=False, 
position_encoding=False, 
position_encoding_type='SinusoidalInterleaved', 
update_vocab=False, 
feat_merge='concat', 
feat_vec_size=-1, 
feat_vec_exponent=0.7, 
model_task='seq2seq', 
model_type='text', 
model_dtype='fp32', 
encoder_type='rnn', 
decoder_type='rnn', 
freeze_encoder=False, 
freeze_decoder=False, 
layers=-1, 
enc_layers=2, 
dec_layers=2, 
hidden_size=-1, 
enc_hid_size=500, 
dec_hid_size=500, 
cnn_kernel_width=3, 
layer_norm='standard', 
pos_ffn_activation_fn='relu', 
input_feed=1, 
bridge=False, 
rnn_type='LSTM', 
context_gate=None, 
bridge_extra_node=True, 
bidir_edges=True, 
state_dim=512, 
n_edge_types=2, 
n_node=2, 
n_steps=2, 
src_ggnn_size=0, 
global_attention='general', 
global_attention_function='softmax', 
self_attn_type='scaled-dot', 
max_relative_positions=0, 
heads=8, 
transformer_ff=2048, 
aan_useffn=False, 
add_qkvbias=False, 
lambda_align=0.0, 
alignment_layer=-3, 
alignment_heads=0, 
full_context_alignment=False, 
copy_attn=False, 
copy_attn_type='general', 
generator_function='softmax', 
copy_attn_force=False, 
reuse_copy_attn=False, 
copy_loss_by_seqlength=False, 
coverage_attn=False, 
lambda_coverage=0.0, 
lm_prior_model=None, 
lm_prior_lambda=0.0, 
lm_prior_tau=1.0, 
loss_scale=0, 
apex_opt_level='', 
data_type='text', 
save_model='model', 
save_checkpoint_steps=5000, 
keep_checkpoint=-1, 
gpu_ranks=[], 
world_size=1, 
gpu_backend='nccl', 
gpu_verbose_level=0, 
master_ip='localhost', 
master_port=10000, 
lora_layers=[], 
lora_embedding=False, 
lora_rank=2, 
lora_alpha=1, 
lora_dropout=0.0, 
quant_layers=[], 
seed=-1, 
param_init=0.1, 
param_init_glorot=False, 
train_from='', 
reset_optim='none', 
pre_word_vecs_enc=None, 
pre_word_vecs_dec=None, 
freeze_word_vecs_enc=False, 
freeze_word_vecs_dec=False, 
num_workers=2, 
batch_size=32, 
batch_size_multiple=1, 
batch_type='sents', 
normalization='sents', 
accum_count=[1], 
accum_steps=[0], 
valid_steps=10000, 
valid_batch_size=32, 
train_steps=100000, 
single_pass=False, 
early_stopping=0, 
early_stopping_criteria=None, 
optim='sgd', 
adagrad_accumulator_init=0, 
max_grad_norm=5, 
dropout=[0.3], 
attention_dropout=[0.1], 
dropout_steps=[0], 
truncated_decoder=0, 
adam_beta1=0.9, 
adam_beta2=0.999, 
label_smoothing=0.0, 
average_decay=0, 
average_every=1, 
learning_rate=1.0, 
learning_rate_decay=0.5, 
start_decay_steps=50000, 
decay_steps=10000, 
decay_method='none',
warmup_steps=4000, 
log_file='', 
log_file_level='0', 
verbose=False, 
train_eval_steps=200, 
train_metrics=[], 
valid_metrics=[], 
scoring_debug=False, 
dump_preds=None, 
report_every=50, 
exp_host='', 
exp='', 
tensorboard=False, 
tensorboard_log_dir='runs/onmt', 
override_opts=False, 
bucket_size=262144, 
bucket_size_init=-1, 
bucket_size_increment=0, 
prefetch_factor=200, 
brnn=False, 
dump_samples=False)


test_onmt.py [-h] [-config CONFIG] [-save_config SAVE_CONFIG] -data
                    DATA [-skip_empty_level {silent,warning,error}]
                    [-transforms {inferfeats,filtertoolong,prefix,suffix,uppercase,sentencepiece,bpe,onmt_tokenize,switchout,tokendrop,tokenmask,docify,clean,inlinetags,fuzzymatch,bart,normalize} [{inferfeats,filtertoolong,prefix,suffix,uppercase,sentencepiece,bpe,onmt_tokenize,switchout,tokendrop,tokenmask,docify,clean,inlinetags,fuzzymatch,bart,normalize} ...]]
                    [-save_data SAVE_DATA] [-overwrite] [-n_sample N_SAMPLE]
                    [-dump_transforms] -src_vocab SRC_VOCAB
                    [-tgt_vocab TGT_VOCAB] [-share_vocab]
                    [--decoder_start_token DECODER_START_TOKEN]
                    [--default_specials DEFAULT_SPECIALS [DEFAULT_SPECIALS ...]]
                    [-n_src_feats N_SRC_FEATS]
                    [-src_feats_defaults SRC_FEATS_DEFAULTS]
                    [-src_vocab_size SRC_VOCAB_SIZE]
                    [-tgt_vocab_size TGT_VOCAB_SIZE]
                    [-vocab_size_multiple VOCAB_SIZE_MULTIPLE]
                    [-src_words_min_frequency SRC_WORDS_MIN_FREQUENCY]
                    [-tgt_words_min_frequency TGT_WORDS_MIN_FREQUENCY]
                    [--src_seq_length_trunc SRC_SEQ_LENGTH_TRUNC]
                    [--tgt_seq_length_trunc TGT_SEQ_LENGTH_TRUNC]
                    [-both_embeddings BOTH_EMBEDDINGS]
                    [-src_embeddings SRC_EMBEDDINGS]
                    [-tgt_embeddings TGT_EMBEDDINGS]
                    [-embeddings_type {GloVe,word2vec}]
                    [--reversible_tokenization {joiner,spacer}]
                    [--src_seq_length SRC_SEQ_LENGTH]
                    [--tgt_seq_length TGT_SEQ_LENGTH]
                    [--src_prefix SRC_PREFIX] [--tgt_prefix TGT_PREFIX]
                    [--src_suffix SRC_SUFFIX] [--tgt_suffix TGT_SUFFIX]
                    [--upper_corpus_ratio UPPER_CORPUS_RATIO]
                    [-src_subword_model SRC_SUBWORD_MODEL]
                    [-tgt_subword_model TGT_SUBWORD_MODEL]
                    [-src_subword_nbest SRC_SUBWORD_NBEST]
                    [-tgt_subword_nbest TGT_SUBWORD_NBEST]
                    [-src_subword_alpha SRC_SUBWORD_ALPHA]
                    [-tgt_subword_alpha TGT_SUBWORD_ALPHA]
                    [-src_subword_vocab SRC_SUBWORD_VOCAB]
                    [-tgt_subword_vocab TGT_SUBWORD_VOCAB]
                    [-src_vocab_threshold SRC_VOCAB_THRESHOLD]
                    [-tgt_vocab_threshold TGT_VOCAB_THRESHOLD]
                    [-src_subword_type {none,sentencepiece,bpe}]
                    [-tgt_subword_type {none,sentencepiece,bpe}]
                    [-src_onmttok_kwargs SRC_ONMTTOK_KWARGS]
                    [-tgt_onmttok_kwargs TGT_ONMTTOK_KWARGS] [--gpt2_pretok]
                    [-switchout_temperature SWITCHOUT_TEMPERATURE]
                    [-tokendrop_temperature TOKENDROP_TEMPERATURE]
                    [-tokenmask_temperature TOKENMASK_TEMPERATURE]
                    [--doc_length DOC_LENGTH] [--max_context MAX_CONTEXT]
                    [--src_eq_tgt] [--same_char] [--same_word]
                    [--scripts_ok [SCRIPTS_OK ...]]
                    [--scripts_nok [SCRIPTS_NOK ...]]
                    [--src_tgt_ratio SRC_TGT_RATIO]
                    [--avg_tok_min AVG_TOK_MIN] [--avg_tok_max AVG_TOK_MAX]
                    [--langid [LANGID ...]]
                    [--tags_dictionary_path TAGS_DICTIONARY_PATH]
                    [--tags_corpus_ratio TAGS_CORPUS_RATIO]
                    [--max_tags MAX_TAGS] [--paired_stag PAIRED_STAG]
                    [--paired_etag PAIRED_ETAG] [--isolated_tag ISOLATED_TAG]
                    [--src_delimiter SRC_DELIMITER] [--tm_path TM_PATH]
                    [--fuzzy_corpus_ratio FUZZY_CORPUS_RATIO]
                    [--fuzzy_threshold FUZZY_THRESHOLD]
                    [--tm_delimiter TM_DELIMITER] [--fuzzy_token FUZZY_TOKEN]
                    [--fuzzymatch_min_length FUZZYMATCH_MIN_LENGTH]
                    [--fuzzymatch_max_length FUZZYMATCH_MAX_LENGTH]
                    [--permute_sent_ratio PERMUTE_SENT_RATIO]
                    [--rotate_ratio ROTATE_RATIO]
                    [--insert_ratio INSERT_RATIO]
                    [--random_ratio RANDOM_RATIO] [--mask_ratio MASK_RATIO]
                    [--mask_length {subword,word,span-poisson}]
                    [--poisson_lambda POISSON_LAMBDA]
                    [--replace_length {-1,0,1}] [--src_lang SRC_LANG]
                    [--tgt_lang TGT_LANG] [--penn PENN]
                    [--norm_quote_commas NORM_QUOTE_COMMAS]
                    [--norm_numbers NORM_NUMBERS]
                    [--pre_replace_unicode_punct PRE_REPLACE_UNICODE_PUNCT]
                    [--post_remove_control_chars POST_REMOVE_CONTROL_CHARS]
                    [--src_word_vec_size SRC_WORD_VEC_SIZE]
                    [--tgt_word_vec_size TGT_WORD_VEC_SIZE]
                    [--word_vec_size WORD_VEC_SIZE]
                    [--share_decoder_embeddings] [--share_embeddings]
                    [--position_encoding]
                    [--position_encoding_type {SinusoidalInterleaved,SinusoidalConcat}]
                    [-update_vocab] [--feat_merge {concat,sum,mlp}]
                    [--feat_vec_size FEAT_VEC_SIZE]
                    [--feat_vec_exponent FEAT_VEC_EXPONENT]
                    [-model_task {seq2seq,lm}] [--model_type {text}]
                    [--model_dtype {fp32,fp16}]
                    [--encoder_type {rnn,brnn,ggnn,mean,transformer,cnn,transformer_lm}]
                    [--decoder_type {rnn,transformer,cnn,transformer_lm}]
                    [--freeze_encoder] [--freeze_decoder] [--layers LAYERS]
                    [--enc_layers ENC_LAYERS] [--dec_layers DEC_LAYERS]
                    [--hidden_size HIDDEN_SIZE] [--enc_hid_size ENC_HID_SIZE]
                    [--dec_hid_size DEC_HID_SIZE]
                    [--cnn_kernel_width CNN_KERNEL_WIDTH]
                    [--layer_norm {standard,rms}]
                    [--pos_ffn_activation_fn {relu,gelu,silu}]
                    [--input_feed INPUT_FEED] [--bridge]
                    [--rnn_type {LSTM,GRU,SRU}]
                    [--context_gate {source,target,both}]
                    [--bridge_extra_node BRIDGE_EXTRA_NODE]
                    [--bidir_edges BIDIR_EDGES] [--state_dim STATE_DIM]
                    [--n_edge_types N_EDGE_TYPES] [--n_node N_NODE]
                    [--n_steps N_STEPS] [--src_ggnn_size SRC_GGNN_SIZE]
                    [--global_attention {dot,general,mlp,none}]
                    [--global_attention_function {softmax,sparsemax}]
                    [--self_attn_type SELF_ATTN_TYPE]
                    [--max_relative_positions MAX_RELATIVE_POSITIONS]
                    [--heads HEADS] [--transformer_ff TRANSFORMER_FF]
                    [--aan_useffn] [--add_qkvbias]
                    [--lambda_align LAMBDA_ALIGN]
                    [--alignment_layer ALIGNMENT_LAYER]
                    [--alignment_heads ALIGNMENT_HEADS]
                    [--full_context_alignment] [--copy_attn]
                    [--copy_attn_type {dot,general,mlp,none}]
                    [--generator_function {softmax,sparsemax}]
                    [--copy_attn_force] [--reuse_copy_attn]
                    [--copy_loss_by_seqlength] [--coverage_attn]
                    [--lambda_coverage LAMBDA_COVERAGE]
                    [--lm_prior_model LM_PRIOR_MODEL]
                    [--lm_prior_lambda LM_PRIOR_LAMBDA]
                    [--lm_prior_tau LM_PRIOR_TAU] [--loss_scale LOSS_SCALE]
                    [--apex_opt_level {,O0,O1,O2,O3}] [--data_type DATA_TYPE]
                    [--save_model SAVE_MODEL]
                    [--save_checkpoint_steps SAVE_CHECKPOINT_STEPS]
                    [--keep_checkpoint KEEP_CHECKPOINT]
                    [--gpu_ranks [GPU_RANKS ...]] [--world_size WORLD_SIZE]
                    [--gpu_backend GPU_BACKEND]
                    [--gpu_verbose_level GPU_VERBOSE_LEVEL]
                    [--master_ip MASTER_IP] [--master_port MASTER_PORT]
                    [--lora_layers LORA_LAYERS [LORA_LAYERS ...]]
                    [--lora_embedding] [--lora_rank LORA_RANK]
                    [--lora_alpha LORA_ALPHA] [--lora_dropout LORA_DROPOUT]
                    [--quant_layers QUANT_LAYERS [QUANT_LAYERS ...]]
                    [--seed SEED] [--param_init PARAM_INIT]
                    [--param_init_glorot] [--train_from TRAIN_FROM]
                    [--reset_optim {none,all,states,keep_states}]
                    [--pre_word_vecs_enc PRE_WORD_VECS_ENC]
                    [--pre_word_vecs_dec PRE_WORD_VECS_DEC]
                    [--freeze_word_vecs_enc] [--freeze_word_vecs_dec]
                    [--num_workers NUM_WORKERS] [--batch_size BATCH_SIZE]
                    [--batch_size_multiple BATCH_SIZE_MULTIPLE]
                    [--batch_type {sents,tokens}]
                    [--normalization {sents,tokens}]
                    [--accum_count ACCUM_COUNT [ACCUM_COUNT ...]]
                    [--accum_steps ACCUM_STEPS [ACCUM_STEPS ...]]
                    [--valid_steps VALID_STEPS]
                    [--valid_batch_size VALID_BATCH_SIZE]
                    [--train_steps TRAIN_STEPS] [--single_pass]
                    [--early_stopping EARLY_STOPPING]
                    [--early_stopping_criteria [EARLY_STOPPING_CRITERIA ...]]
                    [--optim {sgd,adagrad,adadelta,adam,sparseadam,adafactor,fusedadam}]
                    [--adagrad_accumulator_init ADAGRAD_ACCUMULATOR_INIT]
                    [--max_grad_norm MAX_GRAD_NORM]
                    [--dropout DROPOUT [DROPOUT ...]]
                    [--attention_dropout ATTENTION_DROPOUT [ATTENTION_DROPOUT ...]]
                    [--dropout_steps DROPOUT_STEPS [DROPOUT_STEPS ...]]
                    [--truncated_decoder TRUNCATED_DECODER]
                    [--adam_beta1 ADAM_BETA1] [--adam_beta2 ADAM_BETA2]
                    [--label_smoothing LABEL_SMOOTHING]
                    [--average_decay AVERAGE_DECAY]
                    [--average_every AVERAGE_EVERY]
                    [--learning_rate LEARNING_RATE]
                    [--learning_rate_decay LEARNING_RATE_DECAY]
                    [--start_decay_steps START_DECAY_STEPS]
                    [--decay_steps DECAY_STEPS]
                    [--decay_method {noam,noamwd,rsqrt,none}]
                    [--warmup_steps WARMUP_STEPS] [--log_file LOG_FILE]
                    [--log_file_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET,50,40,30,20,10,0}]
                    [--verbose] [--train_eval_steps TRAIN_EVAL_STEPS]
                    [--train_metrics TRAIN_METRICS [TRAIN_METRICS ...]]
                    [--valid_metrics VALID_METRICS [VALID_METRICS ...]]
                    [--scoring_debug] [--dump_preds DUMP_PREDS]
                    [--report_every REPORT_EVERY] [--exp_host EXP_HOST]
                    [--exp EXP] [--tensorboard]
                    [--tensorboard_log_dir TENSORBOARD_LOG_DIR]
                    [--override_opts] [-bucket_size BUCKET_SIZE]
                    [-bucket_size_init BUCKET_SIZE_INIT]
                    [-bucket_size_increment BUCKET_SIZE_INCREMENT]
                    [-prefetch_factor PREFETCH_FACTOR]


---



...